
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8" />
    <title>Diffusion Policy | 跬步千里</title>
    <meta name="author" content="chadwick-yao" />
    <meta name="description" content="" />
    <meta name="keywords" content="" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" />
    <link rel="icon" href="/images/avatar.jpg" />
    <link rel="preconnect" href="https://cdn.staticfile.org" />
<script src="https://cdn.staticfile.org/vue/3.3.7/vue.global.prod.min.js"></script>
<link rel="stylesheet" href="https://cdn.staticfile.org/font-awesome/6.4.2/css/all.min.css" />
<link rel="preconnect" href="https://fonts.loli.net" />
<link rel="preconnect" href="https://gstatic.loli.net" crossorigin />
<link rel="stylesheet" href="https://fonts.loli.net/css2?family=Fira+Code:wght@400;500;600;700&family=Lexend:wght@400;500;600;700;800;900&family=Noto+Sans+SC:wght@400;500;600;700;800;900&display=swap" />
<script> const mixins = {}; </script>

<script src="https://polyfill.io/v3/polyfill.min.js?features=default"></script>


<script src="https://cdn.staticfile.org/highlight.js/11.9.0/highlight.min.js"></script>
<script src="https://cdn.staticfile.org/highlightjs-line-numbers.js/2.8.0/highlightjs-line-numbers.min.js"></script>
<link
    rel="stylesheet"
    href="https://cdn.staticfile.org/highlight.js/11.9.0/styles/github.min.css"
/>
<script src="/js/lib/highlight.js"></script>


<script src="https://cdn.staticfile.org/KaTeX/0.16.9/katex.min.js"></script>
<script src="https://cdn.staticfile.org/KaTeX/0.16.9/contrib/auto-render.min.js"></script>
<link rel="stylesheet" href="https://cdn.staticfile.org/KaTeX/0.16.9/katex.min.css" />
<script src="/js/lib/math.js"></script>


<script src="/js/lib/preview.js"></script>









<script src="https://cdn.staticfile.org/animejs/3.2.1/anime.min.js"></script>
<link rel="stylesheet" href="/css/main.css" />
<link rel="preconnect" href="https://static-argvchs.netlify.app" />

        <link rel="stylesheet" href="/css/mouse.css">
        <script src="https://unpkg.com/@lottiefiles/lottie-player@latest/dist/lottie-player.js"></script>
<meta name="generator" content="Hexo 6.3.0"></head>
<body>
    <div id="layout">
        <transition name="fade">
            <div id="loading" v-show="loading">
                <div id="loading-circle">
                    <h2>LOADING</h2>
                    <p>Wait For a Moment...</p>
                    <img src="/images/loading.gif" />
                </div>
            </div>
        </transition>
        <div id="menu" :class="{ hidden: hiddenMenu, 'menu-color': menuColor}">
    <nav id="desktop-menu">
        <a class="title" href="/">
            <span>跬步千里</span>
        </a>
        
        <a href="/">
            <i class="fa-solid fa-house fa-fw"></i>
            <span>&ensp;Home</span>
        </a>
        
        <a href="/about">
            <i class="fa-solid fa-id-card fa-fw"></i>
            <span>&ensp;About</span>
        </a>
        
        <a href="/archives">
            <i class="fa-solidj fa-box-archive fa-fw"></i>
            <span>&ensp;Archives</span>
        </a>
        
        <a href="/categories">
            <i class="fa-solid fa-bookmark fa-fw"></i>
            <span>&ensp;Categories</span>
        </a>
        
        <a href="/tags">
            <i class="fa-solid fa-tags fa-fw"></i>
            <span>&ensp;Tags</span>
        </a>
        
    </nav>
    <nav id="mobile-menu">
        <div class="title" @click="showMenuItems = !showMenuItems">
            <i class="fa-solid fa-bars fa-fw"></i>
            <span>&emsp;跬步千里</span>
        </div>
        <transition name="slide">
            <div class="items" v-show="showMenuItems">
                
                <a href="/">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-house fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">Home</div>
                    </div>
                </a>
                
                <a href="/about">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-id-card fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">About</div>
                    </div>
                </a>
                
                <a href="/archives">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solidj fa-box-archive fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">Archives</div>
                    </div>
                </a>
                
                <a href="/categories">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-bookmark fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">Categories</div>
                    </div>
                </a>
                
                <a href="/tags">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-tags fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">Tags</div>
                    </div>
                </a>
                
            </div>
        </transition>
    </nav>
</div>
<transition name="fade">
    <div id="menu-curtain" @click="showMenuItems = !showMenuItems" v-show="showMenuItems"></div>
</transition>

        <div id="main" :class="loading ? 'into-enter-from': 'into-enter-active'">
            <div class="article">
    <div>
        <h1>Diffusion Policy</h1>
    </div>
    <div class="info">
        <span class="date">
            <span class="icon">
                <i class="fa-solid fa-calendar fa-fw"></i>
            </span>
            2023/8/22
        </span>
        
        
        <span class="tags">
            <span class="icon">
                <i class="fa-solid fa-tags fa-fw"></i>
            </span>
            
            
            <span class="tag">
                
                <a href="/tags/Imitation-Learning/" style="color: #00bcd4">Imitation Learning</a>
            </span>
            
            <span class="tag">
                
                <a href="/tags/Reinforcement-Learning/" style="color: #ffa2c4">Reinforcement Learning</a>
            </span>
            
        </span>
        
    </div>
    
    <div class="content" v-pre>
        <p>Diffusion model has already been used in computer vision for such a long time. However, in RSS 2023, it’s also used in Robotics, achieving a promising performance.</p>
<span id="more"></span>

<div align="center">
    <img src="/2023/08/22/Diffusion-Policy/gif_1.gif">
</div>

<h1 id="Diffusion-Policy"><a href="#Diffusion-Policy" class="headerlink" title="Diffusion Policy"></a>Diffusion Policy</h1><h2 id="Data"><a href="#Data" class="headerlink" title="Data"></a>Data</h2><p>:arrow_forward: <strong>Original Data in HDF5 File</strong></p>
<p>The official provides dataset in <code>hdf5</code> format. The <code>hdf5</code> file saves many stuffs of demonstrations, such as actions, dones(whether the episode is done), rewards, states(vectors describing the robot states), and obs. However, for training the policy, here just uses a part of this dataset.</p>
<pre><code class="yaml">Group: /data/demo_0
  Dataset: /data/demo_0/actions    shape: (127, 7)
  Dataset: /data/demo_0/dones    shape: (127,)
  Dataset: /data/demo_0/rewards    shape: (127,)
  Dataset: /data/demo_0/states    shape: (127, 45)
  Group: /data/demo_0/next_obs
    Dataset: /data/demo_0/next_obs/agentview_image    shape: (127, 84, 84, 3)
    Dataset: /data/demo_0/next_obs/object    shape: (127, 14)
    Dataset: /data/demo_0/next_obs/robot0_eef_pos    shape: (127, 3)
    Dataset: /data/demo_0/next_obs/robot0_eef_quat    shape: (127, 4)
    Dataset: /data/demo_0/next_obs/robot0_eef_vel_ang    shape: (127, 3)
    Dataset: /data/demo_0/next_obs/robot0_eef_vel_lin    shape: (127, 3)
    Dataset: /data/demo_0/next_obs/robot0_eye_in_hand_image    shape: (127, 84, 84, 3)
    Dataset: /data/demo_0/next_obs/robot0_gripper_qpos    shape: (127, 2)
    Dataset: /data/demo_0/next_obs/robot0_gripper_qvel    shape: (127, 2)
    Dataset: /data/demo_0/next_obs/robot0_joint_pos    shape: (127, 7)
    Dataset: /data/demo_0/next_obs/robot0_joint_pos_cos    shape: (127, 7)
    Dataset: /data/demo_0/next_obs/robot0_joint_pos_sin    shape: (127, 7)
    Dataset: /data/demo_0/next_obs/robot0_joint_vel    shape: (127, 7)
  Group: /data/demo_0/obs
    Dataset: /data/demo_0/obs/agentview_image    shape: (127, 84, 84, 3)
    Dataset: /data/demo_0/obs/object    shape: (127, 14)
    Dataset: /data/demo_0/obs/robot0_eef_pos    shape: (127, 3)
    Dataset: /data/demo_0/obs/robot0_eef_quat    shape: (127, 4)
    Dataset: /data/demo_0/obs/robot0_eef_vel_ang    shape: (127, 3)
    Dataset: /data/demo_0/obs/robot0_eef_vel_lin    shape: (127, 3)
    Dataset: /data/demo_0/obs/robot0_eye_in_hand_image    shape: (127, 84, 84, 3)
    Dataset: /data/demo_0/obs/robot0_gripper_qpos    shape: (127, 2)
    Dataset: /data/demo_0/obs/robot0_gripper_qvel    shape: (127, 2)
    Dataset: /data/demo_0/obs/robot0_joint_pos    shape: (127, 7)
    Dataset: /data/demo_0/obs/robot0_joint_pos_cos    shape: (127, 7)
    Dataset: /data/demo_0/obs/robot0_joint_pos_sin    shape: (127, 7)
    Dataset: /data/demo_0/obs/robot0_joint_vel    shape: (127, 7)
</code></pre>
<p>:arrow_forward: <strong>observation</strong> </p>
<p>Here observation includes an agent view image, a robot image from its hand, end effector’s positions and quaternion, and robot gripper positions. </p>
<pre><code class="tex">agentview_image:
  shape: [3, 84, 84]
  type: rgb
robot0_eye_in_hand_image:
  shape: [3, 84, 84]
  type: rgb
robot0_eef_pos:
  shape: [3]
  # type default: low_dim
robot0_eef_quat:
  shape: [4]
robot0_gripper_qpos:
  shape: [2]
</code></pre>
<p>:arrow_forward: <strong>action</strong></p>
<p>The first three dimension of action is to describe end effector’s position change, and the subsequent three dimension is to illustrate rotation change, and the last dimension is to record gripper’s status.</p>
<pre><code>desired translation of EEF(3), desired delta rotation from current EEF(3), and opening and closing of the gripper fingers:
    shape: [7]
</code></pre>
<h2 id="HyperParameters"><a href="#HyperParameters" class="headerlink" title="HyperParameters"></a>HyperParameters</h2><p>DDMP algorithm hyperparameters of policy, it can affect the denoising performance.</p>
<table>
<thead>
<tr>
<th>name</th>
<th>definition</th>
<th>value</th>
</tr>
</thead>
<tbody><tr>
<td>beta_start</td>
<td>the starting beta value of inference</td>
<td>0.0001</td>
</tr>
<tr>
<td>beta_end</td>
<td>the final beta value</td>
<td>0.02</td>
</tr>
<tr>
<td>beta_schedule</td>
<td>the beta schedule, a mapping from a beta range to a sequence of betas for stepping the model</td>
<td>squaredcos_cap_v2</td>
</tr>
</tbody></table>
<p>Task configuration of policy.</p>
<table>
<thead>
<tr>
<th>name</th>
<th>definition</th>
<th>value</th>
</tr>
</thead>
<tbody><tr>
<td>horizon</td>
<td>the step number of predicted action</td>
<td>10</td>
</tr>
<tr>
<td>n_action_steps</td>
<td>the step number of executing action</td>
<td>8</td>
</tr>
<tr>
<td>n_obs_steps</td>
<td>the step number of obs that the model prediction depends</td>
<td>2</td>
</tr>
</tbody></table>
<p>Image processing of policy</p>
<table>
<thead>
<tr>
<th>name</th>
<th>definition</th>
<th>value</th>
</tr>
</thead>
<tbody><tr>
<td>crop_shape</td>
<td>the target image dimension after cropping</td>
<td>10</td>
</tr>
</tbody></table>
<p>Hyperparameters of model(transformer) that the policy uses.</p>
<table>
<thead>
<tr>
<th>name</th>
<th>definition</th>
<th>value</th>
</tr>
</thead>
<tbody><tr>
<td>n_layer</td>
<td>the layer of decoder&#x2F;encoder</td>
<td>8</td>
</tr>
<tr>
<td>n_head</td>
<td>head number of multi-head attention</td>
<td>4</td>
</tr>
<tr>
<td>n_emb</td>
<td>embedding dimension</td>
<td>256</td>
</tr>
<tr>
<td>p_drop_emb</td>
<td>drop prob of nn.Dropout before encoder&#x2F;decoder</td>
<td>0.0</td>
</tr>
<tr>
<td>p_drop_attn</td>
<td>drop prob of nn.Dropout in transformer layer</td>
<td>0.3</td>
</tr>
</tbody></table>
<p>EMA parameters.</p>
<table>
<thead>
<tr>
<th>name</th>
<th>definition</th>
<th>value</th>
</tr>
</thead>
<tbody><tr>
<td>inv_gamma</td>
<td>inverse multiplicative factor of EMA warmup</td>
<td>1.0</td>
</tr>
<tr>
<td>power</td>
<td>exponential factor of EMA warup</td>
<td>0.75</td>
</tr>
<tr>
<td>min_value</td>
<td>the minimum EMA decay rate</td>
<td>0.0</td>
</tr>
<tr>
<td>max_value</td>
<td>the maximum EMA decay rate</td>
<td>0.9999</td>
</tr>
</tbody></table>
<p>dataloader：</p>
<table>
<thead>
<tr>
<th>name</th>
<th>definition</th>
<th>value</th>
</tr>
</thead>
<tbody><tr>
<td>batch_size</td>
<td>batch size</td>
<td>64</td>
</tr>
<tr>
<td>num_workers</td>
<td>number of processes when loading data</td>
<td>8</td>
</tr>
</tbody></table>
<p>optimizer:</p>
<table>
<thead>
<tr>
<th>name</th>
<th>definition</th>
<th>value</th>
</tr>
</thead>
<tbody><tr>
<td>transformer_weight_decay</td>
<td>transformer weight decay</td>
<td>1.0e-3</td>
</tr>
<tr>
<td>obs_encoder_weight_decay</td>
<td>obs encoder weight decay</td>
<td>1.0e-6</td>
</tr>
<tr>
<td>learning_rate</td>
<td>learning rate</td>
<td>1.0e-4</td>
</tr>
<tr>
<td>betas</td>
<td>decay rate of first-order moment and second-order moment</td>
<td>[0.9, 0.95]</td>
</tr>
</tbody></table>
<pre><code class="yaml">policy: # policy configuration
    _target_: DiffusionTransformerHybridImagePolicy # policy type
    
    shape_meta: # observations and actions specification
        obs:
            agentview_image:
                shape: [3, 84, 84]
                type: rgb
            robot0_eye_in_hand_image:
                shape: [3, 84, 84]
                type: rgb
            robot0_eef_pos:
                shape: [3]
                # type default: low_dim
            robot0_eef_quat:
                shape: [4]
            robot0_gripper_qpos:
                shape: [2]
        action: 
            shape: [7]
    
    noise_scheduler: # DDPM algorithm&#39;s hyperparameters
        _target: DDPMScheduler	# algorithm type
        num_train_timesteps: 100
        beta_start: 0.0001
        beta_end: 0.02
        beta_schedule: squaredcos_cap_v2
        variance_type: fixed_small # Yilun&#39;s paper uses fixed_small_log instead, but easy to cause Nan
        clip_sample: True # required when predict_epsilon=False
        prediction_type: epsilon # or sample
    # task cfg
    horizon: 10 # dataset sequence length
    n_action_steps: 8	# number of steps of action will be executed
    n_obs_steps: 2 # the latest steps of observations data as input
    num_inference_steps: 100
    # image cfg
    crop_shape: [76, 76]	# images will be cropped into [76, 76]
    obs_encoder_group_norm: False,
    # arch
    n_layer: 8	# transformer decoder/encoder layer number
    n_cond_layers: 0  # &gt;0: use transformer encoder for cond, otherwise use MLP
    n_head: 4	# head number
    n_emb: 256	# embedding dim (input dim --(emb)--&gt; n_emb)
    p_drop_emb: 0.0	# dropout prob (before encoder&amp;decoder)
    p_drop_attn: 0.3	# encoder_layer dropout prob
    causal_attn: True	# mask or not
    time_as_cond: True # if false, use BERT like encoder only arch, time as input
    obs_as_cond: True

# if ema is true
ema:
    _target_: diffusion_policy.model.diffusion.ema_model.EMAModel
    update_after_step: 0
    inv_gamma: 1.0
    power: 0.75
    min_value: 0.0
    max_value: 0.9999
dataloader:
    batch_size: 64
    num_workers: 8
    shuffle: True
    pin_memory: True
    persistent_workers: False

val_dataloader:
    batch_size: 64
    num_workers: 8
    shuffle: False
    pin_memory: True
    persistent_workers: False

optimizer:
    transformer_weight_decay: 1.0e-3
    obs_encoder_weight_decay: 1.0e-6
    learning_rate: 1.0e-4
    betas: [0.9, 0.95]

training:
    device: &quot;cuda:0&quot;
    seed: 42
    debug: False
    resume: True
    # optimization
    lr_scheduler: cosine
    # Transformer needs LR warmup
    lr_warmup_steps: 10
    num_epochs: 100
    gradient_accumulate_every: 1
    # EMA destroys performance when used with BatchNorm
    # replace BatchNorm with GroupNorm.
    use_ema: True
    # training loop control
    # in epochs
    rollout_every: 10
    checkpoint_every: 10
    val_every: 1
    sample_every: 5
    # steps per epoch
    max_train_steps: null
    max_val_steps: null
    # misc
    tqdm_interval_sec: 1.0
</code></pre>
<h2 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h2><pre><code class="yaml">obs:
    agentview_image:
        shape: [bs, T, 3, 84, 84]
        type: rgb
    robot0_eye_in_hand_image:
        shape: [bs, T, 3, 84, 84]
        type: rgb
    robot0_eef_pos:
        shape: [bs, T, 3]
    robot0_eef_quat:
        shape: [bs, T, 4]
    robot0_gripper_qpos:
        shape: [bs, T, 2]
action: 
    shape: [bs, T, 7]
timesteps:
    shape: [1]
</code></pre>
<p><img src="/2023/08/22/Diffusion-Policy/image_1.png" alt="Overall Structure"></p>
<p>The picture above describes the overall structure of the training process. We need 3 types of inputs whose definition is introduced in the above code box. Before passing to transformer block, we do preprocessing, like adding noise to action, generating timesteps randomly, and using <code>obs_encoder</code> to extract features from observations.</p>
<pre><code class="python">&quot;&quot;&quot; 1. normalize obs &amp; action -&gt; nobs &amp; naction &quot;&quot;&quot;
nobs = self.normalizer.normalize(batch[&#39;obs&#39;])
nactions = self.normalizer[&#39;action&#39;].normalize(batch[&#39;action&#39;])
trajectory = nactions

&quot;&quot;&quot; 2. take the subsequence of the first To in nobs and do feature extraction with obs_encoder &quot;&quot;&quot;
# reshape B, T, ... to B*T
this_nobs = dict_apply(nobs, lambda x: x[:,:To,...].reshape(-1,*x.shape[2:]))
nobs_features = self.obs_encoder(this_nobs)
# reshape back to B, T, Do
cond = nobs_features.reshape(batch_size, To, -1)

&quot;&quot;&quot; 3. add noise to actions &quot;&quot;&quot;
noise = torch.randn(trajectory.shape, device=trajectory.device)
# Add noise to the clean images according to the noise magnitude at each timestep
# (this is the forward diffusion process)
noisy_trajectory = self.noise_scheduler.add_noise(
    trajectory, noise, timesteps)

&quot;&quot;&quot; 4. generate timesteps randomly &quot;&quot;&quot;
bsz = trajectory.shape[0]
# Sample a random timestep for each image
timesteps = torch.randint(
    0, self.noise_scheduler.config.num_train_timesteps, 
    (bsz,), device=trajectory.device
).long()
</code></pre>
<p>Step 2 can be explained in <a href="#visual encoder">Visual Encoder</a>.</p>
<p>Step 3 can be explained in <a href="#add noise">Add Noise</a>.</p>
<h3 id="Visual-Encoder-how-obs-encoder-extract-features-from-obs"><a href="#Visual-Encoder-how-obs-encoder-extract-features-from-obs" class="headerlink" title="Visual Encoder (how obs_encoder extract features from obs)"></a><span id="visual encoder">Visual Encoder</span> (how obs_encoder extract features from obs)</h3><p>In order to get <code>cond</code>, here has a <span id="obs_encoder">obs_encoder</span> to get features from observations, including images and robot states staff. The encoder is from <code>robomimic</code> package, which is a <code>ObservationGroupEncoder</code> class below. This class is designed to process multiple observations, so one of its arguments is <code>observation_group_shapes</code>, which describes shapes of every observation. And here lists one example of this. </p>
<pre><code class="python">&quot;&quot;&quot;
example of observation_group_shapes:

OrderedDict([(&#39;obs&#39;, 
        OrderedDict([
            (&#39;agentview_image&#39;, [3, 84, 84]), 
            (&#39;robot0_eye_in_hand_image&#39;, [3, 84, 84]), 
            (&#39;robot0_eef_pos&#39;, [3]), 
            (&#39;robot0_eef_quat&#39;, [4]), 
            (&#39;robot0_gripper_qpos&#39;, [2])
            ]))])
&quot;&quot;&quot;
class ObservationsGroupEncoder(Module):
    &quot;&quot;&quot;
    This class allows networks to encode multiple observation dictionaries into a single flat, concatenated vector representation.
    It does this by assigning each observation dictionary (observation group) an @ObservationEncoder Object.
    
    This class takes a dictionary of dictionaries, @observation_group_shapes.
    Each key corresponds to an observation group (e.g. &#39;obs&#39;, &#39;subgoal&#39;, &#39;goal&#39;)
    and each OrderedDict should be a map between modalities and expected input shape (e.g. &#123;&#39;image&#39;: (3, 120, 160)&#125;)
    &quot;&quot;&quot;
    def __init__(
        self,
        observation_group_shapes,
        feature_activation=nn.ReLU,
        encoder_kwargs=None,):
        &quot;&quot;&quot;
        Args:
            observation_group_shapes (OrderedDict): a dictionary of dictionaries.
                Each key in this dictionary should specify an observation group,
                and the value should be an OrderedDict that maps modalities to expected shapes.
            
            feature_activation: non-linearity to apply after each obs net - defaults to ReLU.
            
            encoder_kwargs (dict or None): If None, results in default encoder_kwargs being applied.
                Otherwise, should be nested dictionary containing relevant per-modality information for encoder networks.
                
                should be of form:
                
                obs_modality1: dict
                    feature_dimension: int
                    core_class: str
                    core_kwargs: dict
                        ...
                        ...
                    obs_randomizer_class: str
                    obs_randomizer_kwargs: dict
                        ...
                        ...
                obs_modality2: dict
                    ...
            &quot;&quot;&quot;
            self.observation_group_shapes = observation_group_shapes
            # create an observation encoder per observation group
</code></pre>
<p>Because it can process multiple observations, which means that it has multiple networks for different inputs. Here we have 5 observations, so we have 5 networks. Take agentview_image as an example, its network is established with backbone (resnet18) and pool layers. For such robot0_eef_pos low dimension observation, its network is None. Noticeably, every network here will turn the observation into a 2-dim vector, i.e. [batch_size, output_shape]. Finally, we concatenate all outputs in <code>dim=1</code>, so here is [batch_size, 64+64+3+4+2], i.e. [batch_size, 137].</p>
<pre><code class="txt">ObservationEncoder(
    Key(
        name=agentview_image
        shape=[3, 84, 84]
        modality=rgb
        randomizer=CropRandomizer(input_shape=[3, 84, 84], crop_size=[76, 76], num_crops=1)
        net=VisualCore(
          input_shape=[3, 76, 76]
          output_shape=[64]
          backbone_net=ResNet18Conv(input_channel=3, input_coord_conv=False)
          pool_net=SpatialSoftmax(num_kp=32, temperature=1.0, noise=0.0)
        )
        sharing_from=None
    )
    Key(
        name=robot0_eye_in_hand_image
        shape=[3, 84, 84]
        modality=rgb
        randomizer=CropRandomizer(input_shape=[3, 84, 84], crop_size=[76, 76], num_crops=1)
        net=VisualCore(
          input_shape=[3, 76, 76]
          output_shape=[64]
          backbone_net=ResNet18Conv(input_channel=3, input_coord_conv=False)
          pool_net=SpatialSoftmax(num_kp=32, temperature=1.0, noise=0.0)
        )
        sharing_from=None
    )
    Key(
        name=robot0_eef_pos
        shape=[3]
        modality=low_dim
        randomizer=None
        net=None
        sharing_from=None
    )
    Key(
        name=robot0_eef_quat
        shape=[4]
        modality=low_dim
        randomizer=None
        net=None
        sharing_from=None
    )
    Key(
        name=robot0_gripper_qpos
        shape=[2]
        modality=low_dim
        randomizer=None
        net=None
        sharing_from=None
    )
    output_shape=[137]
)
</code></pre>
<p>The Visual Encoder is not pre-trained model, it will be train with transformer at the same time.</p>
<h3 id="Add-Noise"><a href="#Add-Noise" class="headerlink" title="Add Noise"></a><span id="add noise">Add Noise</span></h3><p>The adding noise process can be computed through the formula below.</p>
<p>$x_t&#x3D;\sqrt{\overline{\alpha_t}}x_0+\sqrt{1-\overline{\alpha_t}}\epsilon$</p>
<p>$x_0$ is the original sample, $\epsilon$ is the noise. $\beta_t $ is the forward process variances of timestep $t$. And it has $\alpha_t&#x3D;1-\beta_t$. So the add_noise function can be displayed below.</p>
<pre><code class="python">def add_noise(original_samples, noise, timesteps):
    sqrt_alpha_prod = alphas_cumprod[timesteps] ** 0.5
    sqrt_one_minus_alpha_prod = (1 - alphas_cumprod[timesteps]) ** 0.5
    
    noise_samples = sqrt_alpha_prod * original_samples + sqrt_one_minus_alpha_prod * noise
    return noise_samples
</code></pre>
<h3 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h3><p><img src="/2023/08/22/Diffusion-Policy/image_1.png" alt="Overall Structure"></p>
<p>Transformer based on diffusion policy is actually one noise predictor. Take in noised data with some conditions, it can predict the noise in the data, and then restore its original data. </p>
<p>The transformer can be seen in the blue dash box in the picture above. After data preprocessing, we have noised sample&#x2F;actions, obs_features&#x2F;cond, and timesteps generated randomly as inputs.</p>
<p>Inputs:</p>
<ul>
<li><p><code>sample</code> is a sequence of noised actions.</p>
</li>
<li><p><code>cond</code> denotes the observation feature. </p>
</li>
<li><p><code>timesteps</code> is the number of diffusion steps.</p>
</li>
</ul>
<p><code>Encoder</code> is designed to  encode observation features and timesteps. <code>n_cond_layers</code> is a hyperparameter that can be set in configuration files, and if it’s &gt; 0, the transformer encoder will replace MLP encoder. </p>
<p><code>Decoder</code> takes in noised actions and encoded information, then predicts a noise with the same shape of X&#x2F;sample as output.</p>
<p>Both transformer encoder and decoder are using torch.nn module, and the transformer forward computation is shown in the code box below.</p>
<blockquote>
<p>Its structure is based on minGPT, which is decoder-only. Here it means that the input <code>sample</code> (noised actions) will only being processed by the transformer decoder, which means that it does not need to learning information from <code>sample</code> by encoder, conversely it just needs to do the noise prediction task by decoder. Details are below, encoder is to process <code>cond</code> and <code>timestep</code> only, and decoder is to process <code>sample</code> only.</p>
</blockquote>
<pre><code class="python">&quot;&quot;&quot;
input arguments:
    sample: A sequence of noised actions.
    cond: Observation features.
    timesteps: diffusion step.
&quot;&quot;&quot;
# 1. inputs embedding
time_emb = self.time_emb(timesteps)
input_emb = self.input_emb(sample)
cond_obs_emb = self.cond_obs_emb(cond)

# 2. prepare transformer encoder inputs, including concatenating and adding position embeddings.
cond_embeddings = torch.cat([time_emb, cond_obs_emb], dim=1)
tc = cond_embeddings.shape[1]
position_embeddings = self.cond_pos_emb[
    :, :tc, :
]  # each position maps to a (learnable) vector
x = self.drop(cond_embeddings + position_embeddings)

# 3. process encoder inputs by encoder
x = self.encoder(x)
memory = x

# 4. prepare decoder inputs - embedded sample + position embedding
token_embeddings = input_emb
t = token_embeddings.shape[1]
position_embeddings = self.pos_emb[
    :, :t, :
]  # each position maps to a (learnable) vector
x = self.drop(token_embeddings + position_embeddings)

# 5. using preprocessed sample and condition information to predict noise by decoder
x = self.decoder(
    tgt=x,
    memory=memory,
    tgt_mask=self.mask,
    memory_mask=self.memory_mask
)
</code></pre>
<h3 id="Loss-Function"><a href="#Loss-Function" class="headerlink" title="Loss Function"></a>Loss Function</h3><p>The training loss is below, the goal is to train a policy $\epsilon_{\theta}$ to predict noise accurately:</p>
<p>$Loss &#x3D; MSE(\epsilon^k, \epsilon_{\theta}(O_t, A_t^0+\epsilon^k,k))$</p>
<p>where $\epsilon^k$ is a random noise with appropriate variance for iteration k. $O_t$ is the observation features.</p>
<p>After predicting noise by transformer, we got <code>pred_noise</code>. Then we use it to calculate training loss with the formula above. </p>
<pre><code class="python"># 1. Predict the noise residual
pred_noise = self.model(noisy_trajectory, timesteps, cond)

# 2. calculate training loss
loss = F.mse_loss(pred, target, reduction=&#39;none&#39;)
loss = loss * loss_mask.type(loss.dtype)
loss = reduce(loss, &#39;b ... -&gt; b (...)&#39;, &#39;mean&#39;)
loss = loss.mean()y
</code></pre>
<h2 id="Inference"><a href="#Inference" class="headerlink" title="Inference"></a>Inference</h2><p>After we got a trained policy $\epsilon_{\theta}$. We use the following formula to inference.</p>
<p>$A_t^{k-1}&#x3D;\alpha(A_t^k-\gamma\epsilon_{\theta}(O_t,A_t^k,k)+N(0,\sigma^2I))$</p>
<p>When doing inference&#x2F;testing&#x2F;rollout, it will use predict_action function of policy in <code>env_runner</code>. The difference between inference and training is that, it would not do backward to update parameters, secondly instead of just getting noise to compute loss, it uses <code>noise_scheduler.step</code> to acquire the original trajectory.</p>
<p>First, we introduce how <code>env_runner</code> works. We can simply decompose the simulation process into 2 steps, i.e. running policy to get predicted actions and stepping environment with predicted actions. </p>
<blockquote>
<p>Further, <code>env_runner</code> uses multiprocessing to achieve multiple environment to execute parallelly. And these simulation environments have 2 types - train and test. Here train type means the initial state is from original dataset, and test type means initial state is set randomly with a different seed.</p>
</blockquote>
<pre><code class="python">while not done:
    # run policy
    with torch.no_grad():
        action_dict = policy.predict_action(obs_dict)

    # step env
    env_action = action
    if self.abs_action:
        env_action = self.undo_transform_action(action)

    obs, reward, done, info = env.step(env_action)
    done = np.all(done)
    past_action = action
</code></pre>
<p>Below is the details of how to implement predicting actions. </p>
<pre><code class="python"># 1. randomly generate a trajectory.
trajectory = torch.randn(
    size=condition_data.shape, 
    dtype=condition_data.dtype,
    device=condition_data.device,
    generator=generator)

# 2. set step values
scheduler.set_timesteps(self.num_inference_steps)

# 3. use scheduler.step to get original trajectory in a loop
for t in scheduler.timesteps:
    # predict noise
    model_output = model(trajectory, t, cond)

    # compute previous image: x_t -&gt; x_t-1
    trajectory = scheduler.step(
        model_output, t, trajectory, 
        generator=generator,
        **kwargs
    ).prev_sample    
# finally we get trajectory_0
</code></pre>
<p>Here is the algorithm of <code>noise_scheduler.step</code>. (x_t -&gt; x_t-1)</p>
<pre><code class="python"># 1. compute alphas, betas
# 2. compute predicted original sample from predicted noise also called
# 3. Clip &quot;predicted x_0&quot;
# 4. Compute coefficients for pred_original_sample x_0 and current sample x_t
# 5. Compute predicted previous sample µ_t
pred_prev_sample = pred_original_sample_coeff * pred_original_sample + current_sample_coeff * sample
# 6. Add noise
pred_prev_sample = pred_prev_sample + variance
</code></pre>
<h2 id="Issues-on-Different-Ways-of-Passing-Observations"><a href="#Issues-on-Different-Ways-of-Passing-Observations" class="headerlink" title="Issues on Different Ways of Passing Observations"></a>Issues on Different Ways of Passing Observations</h2><p>We know that observation features is a input of transformer, but actually it has 2 ways to pass observation features.</p>
<p><strong>Regard Observations as Condition</strong></p>
<p>This way means that, the observation features will be processed by transformer encoder first. Then pass it as conditions to decoder for noise prediction.</p>
<pre><code class="python">cond = None
trajectory = nactions

# reshape B, T, ... to B*T
this_nobs = dict_apply(nobs, lambda x: x[:,:To,...].reshape(-1,*x.shape[2:]))
nobs_features = self.obs_encoder(this_nobs)
# reshape back to B, To, Do
cond = nobs_features.reshape(batch_size, To, -1)
</code></pre>
<p>Another way means that it would not pass the observation features to transformer encoder. Instead, it will regard the observation features as a part of sample. Here, we know that <code>cond</code> is set None, which proves that observation features are not passed to transformer encoder.</p>
<pre><code class="python">cond = None
trajectory = nactions

# reshape B, T, ... to B*T
this_nobs = dict_apply(nobs, lambda x: x.reshape(-1, *x.shape[2:]))
nobs_features = self.obs_encoder(this_nobs)
# reshape back to B, T, Do
nobs_features = nobs_features.reshape(batch_size, horizon, -1)
trajectory = torch.cat([nactions, nobs_features], dim=-1).detach()
</code></pre>
<blockquote>
<p>Here is my point. Looking back ACT model structure, we regard it as a conditional VAE because it not only uses latent code from ENCODER to generate, it also uses observations (camera images, joint positions&#x2F;torques). We regard these observations as conditions, because they are all processed by transformer encoder, and then are passed to transformer decoder to impact its decoding process. So the second passing observation way here, may cannot regard observation as conditions. </p>
<p>However, the point of the second way, I guess, is to make the model not just to predict noise of actions, but also predict noise of observations, i.e. restoring actions and observations at the same time. It’s like only such observations, we do such specific actions. To some extent, the actions are connected to observations. </p>
</blockquote>
<h2 id="Comments"><a href="#Comments" class="headerlink" title="Comments"></a>Comments</h2><ol>
<li>Specify the values of alpha, gamma in the denoising process.</li>
</ol>
<blockquote>
<p>gamma is the learning rate. alpha is a weight to denote the importance of noise.</p>
</blockquote>
<ol start="2">
<li>Specify the value of the variance for iteration k (with explanation).</li>
</ol>
<pre><code class="python"># first sample a variance noise
variance_noise = torch.randn(model_output.shape, dtype=model_output.dtype, generator=generator)
# use _get_variance to get variance of timestep k
variance = (1 - alpha_prod_t_prev) / (1 - alpha_prod_t) * self.betas[t]
variance = torch.clamp(variance, min=1e-20)
# finally do 
variance = (self._get_variance(t, predicted_variance=predicted_variance) ** 0.5) * variance_noise
</code></pre>
<ol start="3">
<li>The Visual Encoder is missing. Add it before the diffusion transformer.</li>
</ol>
<blockquote>
<p>done, see <a href="#obs_encoder">obs_encoder</a></p>
</blockquote>
<ol start="4">
<li>The diffusion transformer adopts the architecture from the minGPT (check it out), which is a decoder-only variant of the Transformer. Modify the content accordingly.</li>
</ol>
<blockquote>
<p>See <a href="#Forward Details">Forward Details</a></p>
</blockquote>
<ol start="5">
<li>noised action &#x3D; noised action execution sequence?</li>
</ol>
<blockquote>
<p>No, the shape of noised action is (B, T, Da), but the shape of noised action execution sequence is (B, n_action_steps, Da). Noticeably, T &gt;&#x3D; n_action_steps</p>
</blockquote>
<ol start="6">
<li>What is the format of the observation feature?</li>
</ol>
<blockquote>
<p>First, we take the subsequence of the first To observations and reshape it, and the make it processed by obs_encoder to get nobs_features, finally we do <code>nobs_features.reshape(B, To, -1)</code> to reshape obs features.</p>
</blockquote>
<ol start="7">
<li>What is bs in [bs, horizon, action_dim]? Why the dimension has three situations?</li>
</ol>
<blockquote>
<p>bs means batch_size. Because it needs to consider whether regarding observations as a condition. If no, the shape of the output is like (B, T, Da+Do), which uses impainting method to replace action with obs features. If yes, then consider whether predicting action steps only, output shape is (B, n_action_steps, Da) when predicting action steps only, else (B, T, Da).</p>
</blockquote>

    </div>
    
    
    
    
    <div id="comment">
        <div id="giscus-container" class="giscus"></div>
    </div>
    
    
    
    
</div>

            <footer id="footer">
    <div id="footer-wrap">
        <div>
            &copy;
            2022 - 2024 跬步千里
            <span id="footer-icon">
                <i class="fa-solid fa-font-awesome fa-fw"></i>
            </span>
            &commat;chadwick-yao
        </div>
        <div>
            Based on the <a target="_blank" rel="noopener" href="https://hexo.io">Hexo Engine</a> &amp;
            <a target="_blank" rel="noopener" href="https://github.com/theme-particlex/hexo-theme-particlex">ParticleX Theme</a>
        </div>
        
    </div>
</footer>

        </div>
        
        <transition name="fade">
            <div id="preview" ref="preview" v-show="previewShow">
                <img id="preview-content" ref="previewContent" />
            </div>
        </transition>
        
    </div>
    <script src="/js/main.js"></script>
    <script>
        console.info("Welcome to Shawn's blog!");
        if (!window.hasOwnProperty("ontouchstart")) {
            let html = '<canvas id="fireworks" style="position: fixed; top: 0; left: 0; width: 100vw; height: 100vh; pointer-events: none; z-index: 32767"></canvas><script src="https://static-argvchs.netlify.app/js/fireworks.min.js"><\/script>';
            document.body.append(document.createRange().createContextualFragment(html));
        }
    </script>
    
    
<script
    src="https://giscus.app/client.js"
    data-repo="argvchs/giscus-comments"
    data-repo-id="R_kgDOI_uC-w"
    data-category="Announcements"
    data-category-id="DIC_kwDOI_uC-84CUToF"
    data-mapping="pathname"
    data-strict="1"
    data-reactions-enabled="1"
    data-emit-metadata="0"
    data-input-position="bottom"
    data-theme="https://static-argvchs.netlify.app/css/giscus.css"
    data-lang="zh-CN"
    crossorigin
    async
></script>





    

        <script type="text/javascript"
        color="0, 0, 0" opacity='0.5' zIndex="-1" count="150" src="//cdn.bootcss.com/canvas-nest.js/1.0.0/canvas-nest.min.js">
        </script>


        <div id="L2dCanvas"></div>
        <link rel="stylesheet" href="/css/live2d.min.css">
        <script src="https://cdn.staticfile.org/pixi.js/4.6.1/pixi.min.js"></script>
        <script src="/js/live2d.min.js"></script>
        <script>
            var v = new Viewer({
                width: 240,
                height: 325,
                right: "0",
                bottom: "0",
                basePath: "/model",
                role: "lingbo",
                mobile: true,
            });
        </script>
        
        <canvas id="fireworks" style="position: fixed; top: 0; left: 0; width: 100vw; height: 100vh; pointer-events: none; z-index: 32767"></canvas>
        <script src="https://cdn.staticfile.org/animejs/3.2.1/anime.min.js"></script>
        <script src="/js/fireworks.min.js"></script>
</body>
</html>
